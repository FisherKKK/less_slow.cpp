/**
 *  less_slow_sm70.ptx
 *
 *  Micro-kernels for building a performance-first mindset for CUDA-capable
 *  GPUs using Parallel Thread eXecution (PTX) Intermediate Representation (IR) 
 *  for different generations of Streaming Multiprocessors (SMs) and Tensor
 *  Cores (TCs) for Volta-generation Nvidia GPUs.
 * 
 *  ? You should start at `less_slow.cu` before reading this file.
 *  ? Also read intro to PTX: https://docs.nvidia.com/cuda/parallel-thread-execution/
 *  ? Check the PTX ISA: https://docs.nvidia.com/cuda/pdf/ptx_isa_8.5.pdf
 *  
 *  ! PTX is higher-level than SASS, but still very similar to typical CPU 
 *  ! assembly languages. It has slightly different syntax and semantics for
 *  ! predicates and memory access, but still don't have `for` loops :(
 *  ! As for emojis, we can only use ASCII... CUDA can't JIT-compile UTF-8.
 * 
 *  You can validate this file by asking the Nvidia PTX Assembler to compile it
 *  to `.cubin` for some target architecture:
 * 
 *  $ ptxas -o less_slow_from_ptx.cubin -arch=sm_90 less_slow.ptx
 *  $ cuobjdump -sass less_slow_from_ptx.cubin | grep -i mma
 * 
 *  @section Register File
 * 
 *  GPU code manages registers differently. Hopper tuning guide suggests that
 *  the register file size is 64K 32-bit registers per Streaming Multiprocessor.
 *  The maximum number of registers per thread is 255.
 * 
 *  PTX provides read-only variables visible a special registers like `%tid` for
 *  thread ID, `%ctaid` for block ID, and `%aggr_smem_size` for shared memory,
 *  or `%current_graph_exec` to access the ID of the current graph execution.
 *  To read from them, simple use the `mov` instruction.
 */

.version 6.5                          // PTX version 6.5 is enough for Volta GPUs
.target sm_70                         // Target architecture (SM 7.0 - Volta GPUs)
.address_size 64                      // 64-bit addressing

.visible .entry tops_f16f16_sm70tc_16x16x16_1024loop_ptx_kernel()
{
    // Register declarations
    .reg .b32  %f<20>;                // Floating-point registers
    .reg .b32  %r<3>;                 // General-purpose registers for loop counter and packed inputs
    .reg .pred %p;                    // Predicate register for conditional branching

    // Initialize the loop counter; but keep in mind, that an algorithm with a
    // loop can be a lot slower than PTX kernels generated by the CUDA compiler.
    // Those end up unrolling thousands of iterations into a single kernel!
    mov.u32    %r0, 0;                // %r0 = loop counter, start at 0
    mov.u32    %r1, 1024;             // %r1 = loop limit (1024 iterations)

    // Zero-initialize accumulators
    mov.f32    %f0, 0.0;              // Initialize %f0 to zero
    mov.f32    %f1, 0.0;              // Initialize %f1 to zero
    mov.f32    %f2, 0.0;              // Initialize %f2 to zero
    mov.f32    %f3, 0.0;              // Initialize %f3 to zero

    // Packed matrix inputs (example placeholders)
    mov.b32    %r2, 0x00010001;       // Packed 16-bit values (placeholder for matrix A and B)

    // Loop start
loop_start:
    setp.ge.u32 %p, %r0, %r1;         // Compare loop counter (%r0) with limit (%r1)
    @%p bra loop_exit;                // Exit loop if %r0 >= %r1

    // Tensor Core MMA operation
	wmma.mma.sync.aligned.row.col.m16n16k16.f16.f16 
        {%f0, %f1, %f2, %f3},                               // Accumulators (output)
        {%f4,  %f5,  %f6,  %f7,  %f8,  %f9,  %f10, %f11},   // Matrix A (packed 16-bit values)
        {%f12, %f13, %f14, %f15, %f16, %f17, %f18, %f19},   // Matrix B (packed 16-bit values)
        {%f0, %f1, %f2, %f3};                               // Accumulators (input)

    // Increment loop counter
    add.u32    %r0, %r0, 1;           // %r0 = %r0 + 1

    // Loop back
    bra        loop_start;

loop_exit:
    ret;                              // Return from kernel
}

/**
 *  Here are some potentially counterintuitive facts about PTX and SASS:
 *
 *  - NVCC unrolls loops more aggressively than any other mainstream compiler.
 *
 *  - If you are coming from CPU side, you shouldn't expect instructions to be
 *    forward-compatible or to have better or equal performance on the next
 *    generation! Entire instruction families may live for just one generation
 *    and be completely abandoned in a couple of years.
 *
 *  - Some instructions, like Tensor Core Gen 4 and 5 operations can't work
 *    with both multiplication operands in GPU registers. At least one of them
 *    has to be in the shared memory. Moreover, they may work up to 10% faster
 *    with both arguments in shared memory!
 *
 *  Because only one `.version` directive can be placed in each file, for newer
 *  kernels, go to `less_slow_sm90a.ptx`.
 */
